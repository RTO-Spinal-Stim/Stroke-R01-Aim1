{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec02c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Environment Configuration\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "# Limit threads for reproducibility\n",
    "os.environ[\"OMP_NUM_THREADS\"]       = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]       = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]   = \"1\"\n",
    "os.environ[\"RAY_LOG_TO_STDERR\"]     = \"1\"\n",
    "os.environ[\"RAY_DISABLE_LOG_MONITOR\"] = \"1\"\n",
    "os.environ[\"RAY_DASHBOARD_DISABLE\"]   = \"1\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"ray\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\".*SAMME.R.*\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Silence Ray logs\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "\n",
    "# Core Libraries\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import (\n",
    "    LeaveOneGroupOut,\n",
    "    StratifiedGroupKFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier as BRFC\n",
    "\n",
    "# Ray / Hyperparameter Search\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CheckpointConfig\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from optuna.samplers import TPESampler\n",
    "from ray.tune import with_parameters\n",
    "from ray.train import report\n",
    "\n",
    "\n",
    "print(\"Ray version:\", ray.__version__)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d544f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning + Feature Selection (Balanced Random Forest)\n",
    "def BRFC_Tune(X_train, y_train, seed_state, splitter, \n",
    "              num_samples=100, n_startup_trials=20, \n",
    "              subject_id=None):\n",
    "\n",
    "    base_storage_dir = r\"C:\\Users\\akishta03\\Documents\\SS_ML\\Ray_Temp\"\n",
    "    storage_dir = os.path.join(base_storage_dir, f\"losocv_subject_{subject_id}\") if subject_id else base_storage_dir\n",
    "\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(\n",
    "            num_cpus=20,\n",
    "            ignore_reinit_error=True,\n",
    "            log_to_driver=False,\n",
    "            include_dashboard=False,\n",
    "            local_mode=False\n",
    "        )\n",
    "\n",
    "    def trainable(config, X, y, cv):\n",
    "        model = BRFC(\n",
    "            n_estimators=config[\"n_estimators\"],\n",
    "            max_depth=config[\"max_depth\"],\n",
    "            min_samples_split=config[\"min_samples_split\"],\n",
    "            min_samples_leaf=config[\"min_samples_leaf\"],\n",
    "            max_features='log2',\n",
    "            sampling_strategy=\"auto\",\n",
    "            #class_weight=\"balanced\",\n",
    "            replacement=True,\n",
    "            random_state=seed_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "        k = scores.size\n",
    "        mean_auc = float(np.mean(scores))\n",
    "        std_auc = float(np.std(scores, ddof=1))\n",
    "        se_auc = float(std_auc / np.sqrt(k))\n",
    "        score = float(mean_auc - se_auc)\n",
    "\n",
    "        report({\"mean_AUC\": mean_auc, \"score\": score})\n",
    "\n",
    "    #Parameter Grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": tune.choice([50, 100, 150, 200, 300]),\n",
    "        \"max_depth\": tune.choice([None, 5, 10, 15]),\n",
    "        \"min_samples_split\": tune.choice([2, 4, 6, 8]),\n",
    "        \"min_samples_leaf\": tune.choice([1, 2, 3, 4])\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Optuna hyperparameter search\n",
    "    optuna_sampler = TPESampler(seed=seed_state, n_startup_trials=n_startup_trials)\n",
    "    search_algo = ConcurrencyLimiter(\n",
    "        OptunaSearch(sampler=optuna_sampler, metric=\"score\", mode=\"max\"),\n",
    "        max_concurrent=1\n",
    "    )\n",
    "\n",
    "    trainable_fn = with_parameters(trainable, X=X_train, y=y_train, cv=splitter)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        trainable_fn,\n",
    "        config=param_grid,\n",
    "        num_samples=num_samples,\n",
    "        search_alg=search_algo,\n",
    "        resources_per_trial={\"cpu\": 20},\n",
    "        reuse_actors=True,\n",
    "        storage_path=storage_dir,\n",
    "        metric=\"score\",\n",
    "        mode=\"max\",\n",
    "        name=\"brfc_tune\",\n",
    "        trial_dirname_creator=lambda trial: f\"{trial.trial_id[:8]}\",\n",
    "        checkpoint_config=CheckpointConfig(checkpoint_frequency=0)\n",
    "    )\n",
    "\n",
    "    best_config = analysis.get_best_config(metric=\"score\", mode=\"max\")\n",
    "\n",
    "    # Final model fit with best hyperparameters\n",
    "    best_model = BRFC(\n",
    "        n_estimators=best_config[\"n_estimators\"],\n",
    "        max_depth=best_config[\"max_depth\"],\n",
    "        min_samples_split=best_config[\"min_samples_split\"],\n",
    "        min_samples_leaf=best_config[\"min_samples_leaf\"],\n",
    "        max_features='log2',\n",
    "        sampling_strategy=\"auto\",\n",
    "        #class_weight=\"balanced\",\n",
    "        replacement=True,\n",
    "        random_state=seed_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    selected_features = X_train.columns.tolist()\n",
    "\n",
    "    return best_model, selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapid correlation reduction of feature space, uses Cramer's V stats for categorical data and Pearson r for numerical data. Threshold is 0.9. Coreelations derived from training data and deployed on test data\n",
    "def corr_feat_elim(cat_cols, dataset, dataset2):\n",
    "\n",
    "    # Separate categorical and numeric variables\n",
    "    # categorical columns are a vector passed in, and used to separate out numeric cols. One-hot encoding performed as a data processing step\n",
    "    numeric_cols = [col for col in dataset.columns if col not in cat_cols and col != 'SubjectID' and col != 'model_set' and col != 'test_set']\n",
    "    threshold = 0.9\n",
    "\n",
    "    # CATEGORICAL ANALYSIS\n",
    "\n",
    "    # Function to calculate Cramér's V for categorical data\n",
    "    def cramers_v(x, y):\n",
    "        # Generate the contingency table\n",
    "        confusion_matrix = pd.crosstab(x, y)\n",
    "        \n",
    "        # Check for degenerate or empty tables\n",
    "        if confusion_matrix.empty or confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
    "            return 0\n",
    "\n",
    "        # Compute chi-squared statistic\n",
    "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "        n = confusion_matrix.sum().sum()\n",
    "        if n <= 1:  # Avoid invalid sample size\n",
    "            return 0\n",
    "\n",
    "        # Compute Cramér's V components\n",
    "        phi2 = chi2 / n\n",
    "        r, k = confusion_matrix.shape\n",
    "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1) if n > 1 else 0)\n",
    "        rcorr = r - ((r-1)**2)/(n-1) if n > 1 else r\n",
    "        kcorr = k - ((k-1)**2)/(n-1) if n > 1 else k\n",
    "\n",
    "        # Compute denominator and Cramér's V\n",
    "        denominator = min((kcorr-1), (rcorr-1))\n",
    "        if denominator <= 0:  # Avoid invalid denominator\n",
    "            return 0\n",
    "        return np.sqrt(phi2corr / denominator)\n",
    "\n",
    "    # Calculate Cramér's V for each pair of categorical features and generate a matrix\n",
    "    cramers_v_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "    for col1 in cat_cols:\n",
    "        for col2 in cat_cols:\n",
    "            if col1 == col2:\n",
    "                cramers_v_matrix.loc[col1, col2] = 1.0\n",
    "            else:\n",
    "                cramers_v_matrix.loc[col1, col2] = cramers_v(dataset[col1], dataset[col2])\n",
    "\n",
    "\n",
    "    # Iterate through matrix and eliminate correlated features\n",
    "    \n",
    "    to_drop_cat = set()\n",
    "\n",
    "    for col1 in cat_cols:\n",
    "        for col2 in cat_cols:\n",
    "            if col1 != col2 and cramers_v_matrix.loc[col1, col2] > threshold:\n",
    "                to_drop_cat.add(col2)\n",
    "\n",
    "    # Create reconciled datset of categorical features\n",
    "    dataset_cat = dataset[cat_cols].drop(columns=to_drop_cat)\n",
    "    dataset2_cat = dataset2[cat_cols].drop(columns=to_drop_cat)\n",
    "\n",
    "    # NUMERIC ANALYSIS\n",
    "\n",
    "    # Calculate Pearson correlation for numeric features\n",
    "    dataset_num = dataset[numeric_cols].apply(pd.to_numeric, errors = 'coerce') # make sure types are standardized\n",
    "    dataset2_num = dataset2[numeric_cols].apply(pd.to_numeric, errors = 'coerce') # make sure types are standardized\n",
    "    corr_matrix = dataset_num.corr() # pandas implementation on CPU as this calculates pairwise and does not consider missing data\n",
    "\n",
    "\n",
    "    # GPU acceleration of pairwise feature elimination\n",
    "    corr_matrix_gpu = np.asarray(corr_matrix.values)\n",
    "    num_features = corr_matrix_gpu.shape[0]\n",
    "    mask = np.abs(corr_matrix_gpu) > threshold\n",
    "    mask = mask * ~np.eye(mask.shape[0], dtype=bool)\n",
    "    indices = np.nonzero(mask)\n",
    "    indices_np = indices[0], indices[1]\n",
    "\n",
    "    highly_correlated_pairs = set()\n",
    "    for i, j in zip(indices_np[0], indices_np[1]):\n",
    "        if i < j:\n",
    "            highly_correlated_pairs.add(corr_matrix.columns[j])\n",
    "\n",
    "\n",
    "    # Combine the reduced categorical and numeric data\n",
    "\n",
    "    dataset_num = dataset_num.drop(columns = highly_correlated_pairs)\n",
    "    dataset2_num = dataset2_num.drop(columns = highly_correlated_pairs)\n",
    "    full_dataset = pd.concat([dataset_cat, dataset_num], axis=1).reset_index(drop=True)\n",
    "    full_dataset2 = pd.concat([dataset2_cat, dataset2_num], axis=1).reset_index(drop=True)\n",
    "    full_dataset.astype(np.float32) # conversion for GPU accelerations to work downstream\n",
    "    full_dataset2.astype(np.float32) # conversion for GPU accelerations to work downstream\n",
    "\n",
    "    columns_to_drop = full_dataset.filter(like=\"Unnamed\").columns\n",
    "    #print(columns_to_drop)\n",
    "    columns_to_drop2 = full_dataset2.filter(like=\"Unnamed\").columns\n",
    "    #print(columns_to_drop2)\n",
    "\n",
    "    full_dataset = full_dataset.drop(columns = columns_to_drop)\n",
    "    full_dataset2 = full_dataset2.drop(columns = columns_to_drop2)\n",
    "\n",
    "    return full_dataset, full_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0625a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Set Configuration:\n",
    "--------------------------------------------------------------------\n",
    "\"all\"       → Demographics, Pre + (PrePostDiff) Gait Symmetry, and MEPs  \n",
    "\"demo\"      → Demographics only  \n",
    "\"demoMEPs\"  → Demographics and MEPs  \n",
    "\"gait\"      → Pre + (PrePostDiff) Gait Symmetry  \n",
    "\"preGait\"   → Pre only Gait Symmetry  \n",
    "\"preOnly\"   → Demographics, Pre only Gait Symmetry, and MEPs\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data Input Configuration dictionary\n",
    "# ---------------------------------------------------------------------\n",
    "DATA_CONFIG = {\n",
    "    \"all\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_AllData.csv\",\n",
    "        \"cat_cols\": [\n",
    "            \"Sex\", \"Laterality\", \"Type of Stroke\", \"Community Orthotic Type\", \"Orthotic During Session\",\n",
    "            \"Lowest RMT Location\", \"Lowest RMT Paretic\", \"Lowest RMT Flexor\",\n",
    "            \"Last RMT Location\", \"Last RMT Paretic\", \"Last RMT Flexor\"\n",
    "        ],\n",
    "        \"drop_cols\": ['Lowest RMT Muscle','Lowest RMT Muscles', 'Last RMT Muscle']\n",
    "    },\n",
    "    \"demo\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_Demo.csv\",\n",
    "        \"cat_cols\": [\"Sex\", \"Laterality\", \"Type of Stroke\", \"Community Orthotic Type\", \"Orthotic During Session\"],\n",
    "        \"drop_cols\": []\n",
    "    },\n",
    "    \"demoMEPs\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_Demo_MEPs.csv\",\n",
    "        \"cat_cols\": [\n",
    "            \"Sex\", \"Laterality\", \"Type of Stroke\", \"Community Orthotic Type\", \"Orthotic During Session\",\n",
    "            \"Lowest RMT Location\", \"Lowest RMT Paretic\", \"Lowest RMT Flexor\",\n",
    "            \"Last RMT Location\", \"Last RMT Paretic\", \"Last RMT Flexor\"\n",
    "        ],\n",
    "        \"drop_cols\": ['Lowest RMT Muscle','Lowest RMT Muscles', 'Last RMT Muscle']\n",
    "    },\n",
    "    \"MEPs\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_MEPs.csv\",\n",
    "        \"cat_cols\": [\n",
    "            \"Lowest RMT Location\", \"Lowest RMT Paretic\", \"Lowest RMT Flexor\",\n",
    "            \"Last RMT Location\", \"Last RMT Paretic\", \"Last RMT Flexor\"\n",
    "        ],\n",
    "        \"drop_cols\": ['Lowest RMT Muscle','Lowest RMT Muscles', 'Last RMT Muscle']\n",
    "    },\n",
    "    \"gait\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_Gait.csv\",\n",
    "        \"cat_cols\": [],\n",
    "        \"drop_cols\": []\n",
    "    },\n",
    "    \"preGait\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_Pre_Gait.csv\",\n",
    "        \"cat_cols\": [],\n",
    "        \"drop_cols\": []\n",
    "    },\n",
    "    \"preOnly\": {\n",
    "        \"path\": r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\Model_Input_AllFeat_PreOnlyData.csv\",\n",
    "        \"cat_cols\": [\n",
    "            \"Sex\", \"Laterality\", \"Type of Stroke\", \"Community Orthotic Type\", \"Orthotic During Session\",\n",
    "            \"Lowest RMT Location\", \"Lowest RMT Paretic\", \"Lowest RMT Flexor\",\n",
    "            \"Last RMT Location\", \"Last RMT Paretic\", \"Last RMT Flexor\"\n",
    "        ],\n",
    "        \"drop_cols\": ['Lowest RMT Muscle','Lowest RMT Muscles', 'Last RMT Muscle']\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Data Loader function\n",
    "def load_dataset(name: str):\n",
    "    \"\"\"\n",
    "    Load a dataset by nickname and return:\n",
    "        df           : pandas.DataFrame\n",
    "        cat_cols     : list[str] # Categorical columns\n",
    "        drop_cols    : list[str] # Columns to drop\n",
    "    \"\"\"\n",
    "    dataSet = name\n",
    "    cfg = DATA_CONFIG[name]           \n",
    "    csv_path = Path(cfg[\"path\"])\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df, cfg[\"cat_cols\"], cfg[\"drop_cols\"], name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Set Configuration:\n",
    "--------------------------------------------------------------------\n",
    "\"all\"       → Demographics, Pre + (PrePostDiff) Gait Symmetry, and MEPs  \n",
    "\"demo\"      → Demographics only  \n",
    "\"demoMEPs\"  → Demographics and MEPs  \n",
    "\"gait\"      → Pre + (PrePostDiff) Gait Symmetry  \n",
    "\"preGait\"   → Pre only Gait Symmetry  \n",
    "\"preOnly\"   → Demographics, Pre only Gait Symmetry, and MEPs\n",
    "\"\"\"\n",
    "\n",
    "# FINAL MODEL USES \"preOnly\" DATASET\n",
    "# Load input data\n",
    "input_df, cat_cols, drop_cols, dataSet = load_dataset(\"preOnly\")\n",
    "print(f\"Loaded {input_df.shape[0]:,} rows, {input_df.shape[1]} columns.\")\n",
    "print(\"Categorical cols :\", cat_cols)\n",
    "print(\"Drop cols        :\", drop_cols)\n",
    "\n",
    "\n",
    "# Numeric features\n",
    "input_df_num = input_df.drop(columns=cat_cols + drop_cols)\n",
    "\n",
    "# Load outcome labels\n",
    "outcome_df = pd.read_csv(r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Labels and Inputs\\ML_Labels.csv\")\n",
    "\n",
    "\n",
    "# Separate numeric columns\n",
    "input_df_num = input_df.drop(columns=cat_cols + drop_cols)\n",
    "\n",
    "# One-Hot Encoding\n",
    "ohe = OneHotEncoder(drop=None, sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(input_df[cat_cols])\n",
    "\n",
    "# Transform categorical columns\n",
    "cat_encoded = ohe.transform(input_df[cat_cols])\n",
    "cat_encoded_df = pd.DataFrame(cat_encoded, columns=ohe.get_feature_names_out(cat_cols), index=input_df.index)\n",
    "\n",
    "# Combine with numeric and outcome data\n",
    "merged_df_ohe = pd.concat([input_df_num, cat_encoded_df], axis=1)\n",
    "merged_df_ohe = merged_df_ohe.merge(outcome_df, on=\"Subject\", how=\"inner\")\n",
    "\n",
    "# Relace NaNs with -1 for all columns\n",
    "merged_df_ohe = merged_df_ohe.fillna(-1)\n",
    "\n",
    "# Check for NaNs\n",
    "nan_summary = merged_df_ohe.isna().sum()\n",
    "print(\"NaN count per column:\\n\", nan_summary[nan_summary > 0])\n",
    "\n",
    "# Output OHE dataframe\n",
    "merged_df_ohe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzing the key parameters for the pipeline\n",
    "seed = 123 # model and pipeline reproducibility\n",
    "loso = LeaveOneGroupOut() # defining the splitter for LOSCOCV\n",
    "results_df = pd.DataFrame()  # storing outputs for later analysis / plotting. Need to reinitialize before every runthrough\n",
    "outcome_vec = ['Frequency Label','Intensity Label'] # defining what to iterate through in the for-loop\n",
    "cat_features = [c for c in merged_df_ohe.columns if any(cat in c for cat in cat_cols)] # defining cat_features for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize status log path\n",
    "status_log_path = r\"C:\\Users\\akishta03\\Documents\\SS_ML\\status_log.txt\"\n",
    "\n",
    "# Optional: clear old logs if rerunning whole script\n",
    "with open(status_log_path, \"w\") as f:\n",
    "    f.write(\"Run Status Log\\n\")\n",
    "    f.write(\"====================\\n\")\n",
    "\n",
    "# EasyEnsemble + SelectFromModel tuning LOSOCV Model Pipeline\n",
    "# Model results file name, updated with date, and data set type\n",
    "model_name   = \"brfc\"  # \"easyensemble\" or \"brfc\"\n",
    "output_label = f\"{model_name}_{dataSet}_{datetime.now().strftime('%Y%m%d')}\"\n",
    "\n",
    "# Initialize a dataframe to store selected feature names for each LOSOCV run\n",
    "losocv_features = pd.DataFrame()\n",
    "\n",
    "\n",
    "for outcome in outcome_vec:\n",
    "    print(outcome)\n",
    "    iter_df = merged_df_ohe.copy()\n",
    "\n",
    "    for train_idx, test_idx in loso.split(iter_df, groups=iter_df[\"Subject\"]):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_df = iter_df.iloc[train_idx].reset_index(drop=True)\n",
    "        test_df  = iter_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "        subject_test  = test_df[\"Subject\"]\n",
    "        subject_trial = test_df[\"Trial\"]\n",
    "        subject_trial_diff = test_df[\"Trial_Diff\"] if \"Trial_Diff\" in test_df.columns else None\n",
    "        \n",
    "        kfold = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=seed) # defining the splitter for k-fold CV\n",
    "        kfold_splits = list(\n",
    "            kfold.split(train_df, train_df[outcome], groups=train_df[\"Subject\"])\n",
    "        )\n",
    "\n",
    "        train_df = train_df.drop(columns=[\"Subject\"])\n",
    "        test_df  = test_df.drop(columns=[\"Subject\"])\n",
    "\n",
    "        X_train = train_df.drop(columns=[c for c in train_df.columns if \"Label\" in c])\n",
    "        X_test  = test_df.drop(columns=[c for c in test_df.columns if \"Label\" in c])\n",
    "        y_train = train_df[outcome]\n",
    "        y_test  = test_df[outcome]\n",
    "\n",
    "        # Correlation-based elimination\n",
    "        X_train, X_test = corr_feat_elim(cat_features, X_train, X_test)\n",
    "\n",
    "        # Scale numeric data only\n",
    "        numeric_cols = [c for c in X_train.columns if c not in cat_features]\n",
    "        scaler = RobustScaler()\n",
    "        X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "        X_test[numeric_cols]  = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "        # Train model using EasyEnsemble with hyperparameter search in Optuna\n",
    "        print(\"Starting tune for BRFC...\")\n",
    "        final_model, selected_features = BRFC_Tune(X_train, y_train, seed, kfold_splits)\n",
    "        \n",
    "        # --- NEW: Save selected features for this LOSOCV iteration ---\n",
    "        feat_row = pd.Series(selected_features, name=f\"{outcome}_Subj{subject_test.unique()[0]}\")\n",
    "        losocv_features = pd.concat([losocv_features, feat_row], axis=1)\n",
    "\n",
    "        # Evaluate on held-out subject\n",
    "        X_test_sel = X_test[selected_features]\n",
    "        y_pred     = final_model.predict(X_test_sel)\n",
    "        y_prob     = final_model.predict_proba(X_test_sel)[:, 1]\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        status_msg = f\"{output_label} | Outcome: {outcome} | Subject {subject_test.unique()[0]} | Time {elapsed:.2f}s\"\n",
    "        print(status_msg)\n",
    "        \n",
    "        # Write status message to log\n",
    "        with open(status_log_path, \"a\") as f:\n",
    "            f.write(status_msg + \"\\n\")\n",
    "\n",
    "        # Record test results\n",
    "        out_df = pd.DataFrame({\n",
    "            \"Subject\": subject_test,\n",
    "            \"Trial\":   subject_trial,\n",
    "            \"y_true\":  y_test.values.ravel(),\n",
    "            \"y_pred\":  y_pred,\n",
    "            \"y_prob\":  y_prob,\n",
    "        })\n",
    "        if subject_trial_diff is not None:\n",
    "            out_df[\"Trial_Diff\"] = subject_trial_diff\n",
    "        out_df[\"Outcome\"] = outcome\n",
    "        out_df[\"Model\"]   = model_name.upper()\n",
    "\n",
    "\n",
    "        results_df = pd.concat([results_df, out_df], ignore_index=True)\n",
    "\n",
    "    # save results with version increase if file exists\n",
    "base_path   = r'C:\\Users\\akishta03\\Documents\\SS_ML\\ML Results'\n",
    "output_file = f\"{base_path}\\\\{output_label}_Final_100125_Frequency_dropFirst.csv\"\n",
    "ver = 1\n",
    "while os.path.exists(output_file):\n",
    "    output_file = f\"{base_path}\\\\{output_label}_v{ver}.csv\"\n",
    "    ver += 1\n",
    "\n",
    "results_df.to_csv(output_file, index=False)\n",
    "# --- Save LOSOCV feature names to CSV ---\n",
    "feat_output_file = f\"{base_path}\\\\{output_label}_LOSOCV_Features.csv\"\n",
    "losocv_features.to_csv(feat_output_file, index=False)\n",
    "print(f\"Saved LOSOCV feature names to {feat_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf245f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL SUBJECT TRAINING + SHAP ANALYSIS\n",
    "# ============================================================\n",
    "# Train on ALL subjects + Tree SHAP (Top-10)\n",
    "# Save SHAP arrays + global CSV\n",
    "# Rename features for plots using FeatureNameLabelMapping.xlsx\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "model_name    = \"BRFC\"\n",
    "fig_base_path = r\"Y:\\LabMembers\\Ameen\\VScode Scripts\\Spinal Stim Aim 1 Codes\\ML Results\\SHAP\"\n",
    "os.makedirs(fig_base_path, exist_ok=True)\n",
    "\n",
    "# no date tag in cache filenames (so we can reload)\n",
    "shap_label = f\"{model_name}_{dataSet}_ALLSUBJ\"\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"figure.figsize\": (7.0, 5.0),\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"axes.grid\": False,\n",
    "})\n",
    "\n",
    "RANDOM_STATE = seed\n",
    "top_k = 10\n",
    "\n",
    "# x-axis labels\n",
    "x_axis_labels = {\n",
    "    \"Frequency Label\": \"SHAP value (impact on model output)\\n← 50 Hz                               30 Hz →\",\n",
    "    \"Intensity Label\": \"SHAP value (impact on model output)\\n← RMT Intensity               TOL Intensity →\"\n",
    "}\n",
    "default_xlabel = \"SHAP value (impact on model output)\"\n",
    "\n",
    "# Load feature mapping \n",
    "mapping_path = os.path.join(fig_base_path, \"FeatureNameLabelMapping.xlsx\")\n",
    "feature_map = {}\n",
    "if os.path.exists(mapping_path):\n",
    "    mdf = pd.read_excel(mapping_path)\n",
    "    feature_map = dict(zip(mdf[\"Feature Name\"].astype(str), mdf[\"Rename\"].astype(str)))\n",
    "\n",
    "def rename_list(names):\n",
    "    return [feature_map.get(str(n), str(n)) for n in names]\n",
    "\n",
    "\n",
    "# Loop over outcomes\n",
    "for outcome in outcome_vec:\n",
    "    print(f\"\\n=== Train on ALL subjects + SHAP :: Outcome = {outcome} ===\")\n",
    "    xlabel_text = x_axis_labels.get(outcome, default_xlabel)\n",
    "\n",
    "    # Load dataset\n",
    "    all_df = merged_df_ohe.copy()\n",
    "    groups = all_df[\"Subject\"].copy()\n",
    "    kfold = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    kfold_splits_all = list(kfold.split(all_df, all_df[outcome], groups=groups))\n",
    "\n",
    "    all_df = all_df.drop(columns=[\"Subject\"])\n",
    "    X_all = all_df.drop(columns=[c for c in all_df.columns if \"Label\" in c])\n",
    "    y_all = all_df[outcome].astype(int).values\n",
    "\n",
    "    X_all, _ = corr_feat_elim(cat_features, X_all, X_all)\n",
    "    numeric_cols = [c for c in X_all.columns if c not in cat_features]\n",
    "    scaler = RobustScaler()\n",
    "    X_all.loc[:, numeric_cols] = scaler.fit_transform(X_all[numeric_cols])\n",
    "\n",
    "    # Load cached model or train\n",
    "    model_cache_path = os.path.join(\n",
    "        fig_base_path, f\"{model_name}_{dataSet}_ALLSUBJ_{outcome}_final_model.pkl\"\n",
    "    )\n",
    "    if os.path.exists(model_cache_path):\n",
    "        with open(model_cache_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        final_model = data[\"model\"]\n",
    "        selected_features = data[\"features\"]\n",
    "        print(f\"Loaded cached model for {outcome}\")\n",
    "    else:\n",
    "        final_model, selected_features = BRFC_Tune(\n",
    "            X_all, y_all, RANDOM_STATE, kfold_splits_all\n",
    "        )\n",
    "        with open(model_cache_path, \"wb\") as f:\n",
    "            pickle.dump({\"model\": final_model, \"features\": selected_features}, f)\n",
    "        print(f\"Trained and cached model for {outcome}\")\n",
    "\n",
    "    # Slice to selected features\n",
    "    X_sel_all = X_all[selected_features].copy()\n",
    "    if not hasattr(final_model, \"feature_names_in_\"):\n",
    "        final_model.feature_names_in_ = np.array(selected_features, dtype=object)\n",
    "\n",
    "    # Compute or Load SHAP\n",
    "    shap_vals_path = os.path.join(fig_base_path, f\"{shap_label}_{outcome}_SHAP_values.npz\")\n",
    "\n",
    "    if os.path.exists(shap_vals_path):\n",
    "        print(f\"Loading cached SHAP values for {outcome}\")\n",
    "        data = np.load(shap_vals_path, allow_pickle=True)\n",
    "        shap_vals     = np.array(data[\"shap_vals\"])\n",
    "        feature_names = data[\"feature_names\"]\n",
    "        X_values      = data[\"X_values\"]\n",
    "\n",
    "        # Handle binary classification case\n",
    "        if shap_vals.ndim == 3 and shap_vals.shape[-1] == 2:\n",
    "            shap_vals = shap_vals[:, :, 1]  # keep positive class\n",
    "        if isinstance(shap_vals, list) and len(shap_vals) == 2:\n",
    "            shap_vals = shap_vals[1]\n",
    "\n",
    "        # restore into DataFrame\n",
    "        X_sel_all = pd.DataFrame(X_values, columns=feature_names)\n",
    "        selected_features = list(feature_names)\n",
    "    else:\n",
    "        print(f\"Computing SHAP values for {outcome} (TreeExplainer)\")\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model=final_model,\n",
    "            data=X_sel_all,\n",
    "            model_output=\"probability\",         \n",
    "            feature_perturbation=\"interventional\",\n",
    "            feature_names=X_sel_all.columns\n",
    "        )\n",
    "\n",
    "        shap_vals = explainer.shap_values(X_sel_all)\n",
    "\n",
    "        # Binary classification\n",
    "        if isinstance(shap_vals, list) and len(shap_vals) == 2:\n",
    "            shap_vals = shap_vals[1]\n",
    "        elif shap_vals.ndim == 3 and shap_vals.shape[-1] == 2:\n",
    "            shap_vals = shap_vals[:, :, 1]\n",
    "\n",
    "        shap_vals = np.array(shap_vals)\n",
    "        print(\"Final shap_vals shape:\", shap_vals.shape)\n",
    "\n",
    "        # Save SHAP arrays\n",
    "        np.savez_compressed(\n",
    "            shap_vals_path,\n",
    "            shap_vals=shap_vals,\n",
    "            feature_names=np.array(X_sel_all.columns, dtype=object),\n",
    "            X_values=X_sel_all.values\n",
    "        )\n",
    "        print(f\"Saved SHAP values to {shap_vals_path}\")\n",
    "\n",
    "    # Global importance\n",
    "    mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "    nonzero_idx = np.where(mean_abs > 0)[0]\n",
    "\n",
    "    if len(nonzero_idx) > 10:\n",
    "        order = np.argsort(mean_abs[nonzero_idx])[::-1][:10]\n",
    "        top_idx = nonzero_idx[order]\n",
    "    else:\n",
    "        order = np.argsort(mean_abs[nonzero_idx])[::-1]\n",
    "        top_idx = nonzero_idx[order]\n",
    "\n",
    "    top_idx = np.ravel(top_idx)  # flatten to 1D\n",
    "    top_feats = X_sel_all.columns[top_idx].tolist()\n",
    "    display_top_feats = rename_list(top_feats)\n",
    "\n",
    "    shap_global = pd.DataFrame({\n",
    "        \"feature\": X_sel_all.columns,\n",
    "        \"mean_abs_shap\": mean_abs.ravel()\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    shap_global[\"display_name\"] = rename_list(shap_global[\"feature\"])\n",
    "    table_path = os.path.join(fig_base_path, f\"{shap_label}_{outcome}_SHAP_global.csv\")\n",
    "    shap_global.to_csv(table_path, index=False)\n",
    "\n",
    "    # Plots\n",
    "    n_display = len(top_idx)\n",
    "\n",
    "    # Bar\n",
    "    plt.figure()\n",
    "    plt.barh(range(n_display), mean_abs[top_idx], align=\"center\")\n",
    "    plt.yticks(range(n_display), display_top_feats)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(default_xlabel)\n",
    "    plt.title(f\"{model_name} — {outcome} — Mean |SHAP| (Top {n_display})\")\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(fig_base_path, f\"{shap_label}_{outcome}_BAR_top{n_display}.png\")\n",
    "    plt.savefig(bar_path, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    # Beeswarm\n",
    "    X_eval_top = X_sel_all[top_feats].copy()\n",
    "    X_eval_top.columns = display_top_feats\n",
    "    shap_vals_top = shap_vals[:, top_idx]\n",
    "\n",
    "    plt.figure()\n",
    "    shap.summary_plot(\n",
    "        shap_vals_top,\n",
    "        X_eval_top,\n",
    "        show=False,\n",
    "        max_display=n_display\n",
    "    )\n",
    "    # Get the colorbar axis \n",
    "    cbar = plt.gcf().axes[-1]\n",
    "\n",
    "    # Replace labels\n",
    "    cbar.set_yticklabels([\"Low / False\", \"High / True\"])\n",
    "\n",
    "    # Force symmetric x-axis with 0 centered\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    xmax = max(abs(xlim[0]), abs(xlim[1]))\n",
    "    ax.set_xlim(-xmax, xmax)\n",
    "\n",
    "    plt.xlabel(xlabel_text)\n",
    "    plt.title(f\"{outcome} — SHAP Beeswarm (Top {n_display})\")\n",
    "    plt.tight_layout()\n",
    "    beeswarm_path = os.path.join(fig_base_path, f\"{shap_label}_{outcome}_BEESWARM_top{n_display}.png\")\n",
    "    plt.savefig(beeswarm_path, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved: {bar_path}\\n       {beeswarm_path}\\n       {table_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN_ENV (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
